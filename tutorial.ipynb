{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashworks1706/AI-Solar-Panel/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUFXapyFjPJi"
      },
      "source": [
        "# Self Rotatory Solar Panel\n",
        "\n",
        "This Project utilizes various vision models to train on planet sun labelled datasets and perform validation checks to determine which model performs the best and converts the best model to tinyML Model to make it usable by microcontroller"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_xvdAxVjnnZ"
      },
      "source": [
        "## Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpanF62hk2A2"
      },
      "outputs": [],
      "source": [
        "# %pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "QyLqMe3CY_7v",
        "outputId": "58c4cc42-cd62-4f3f-8e54-85e1f94c8ad1"
      },
      "outputs": [],
      "source": [
        "# %pip install inference  suncalc  roboflow ultralytics torch matplotlib geocoder numpy ffmpeg "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install tensorflow tflite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oTtl35YjMFb"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89gL1DtFcJwD",
        "outputId": "49112995-3533-4b71-d1c9-85e6649aa033"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "# import time\n",
        "# import geocoder\n",
        "# import locale\n",
        "# import tensorflow as tf\n",
        "# from roboflow import Roboflow\n",
        "# from suncalc import get_position\n",
        "# import supervision as sv\n",
        "# from datetime import datetime, timezone, timedelta\n",
        "\n",
        "# from inference import get_model\n",
        "\n",
        "# Commented out Google Colab-specific imports\n",
        "# from google.colab import drive\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqb6OqQ-rvTN"
      },
      "source": [
        "## Setting up Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGHeCoOjr5Rn"
      },
      "outputs": [],
      "source": [
        "# Google Colab\n",
        "# api_key = userdata.get('ROBOFLOW_API_KEY')\n",
        "\n",
        "# Local (appConfig.json required)\n",
        "try:\n",
        "    with open('appConfig.json') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    api_key = config.get('ROBOFLOW_API_KEY', '')\n",
        "except FileNotFoundError:\n",
        "    print(\"appConfig.json not found. Please make sure the file exists in the same directory as the script.\")\n",
        "    api_key = ''\n",
        "except json.JSONDecodeError:\n",
        "    print(\"Error decoding appConfig.json. Please make sure it's a valid JSON file.\")\n",
        "    api_key = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5RXRmf7s0Xm",
        "outputId": "26ec4ebe-52d5-43a1-b281-51d55cde2162"
      },
      "outputs": [],
      "source": [
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYgTafGdkgJg"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVsKVdSmkj_b"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ModelEvaluationDisplay:\n",
        "    def __init__(self, metrics, plots_path):\n",
        "        self.metrics = metrics\n",
        "        self.plots_path = plots_path\n",
        "\n",
        "    def display_overall_metrics(self):\n",
        "        mAP50_95 = self.metrics.box.map\n",
        "        mAP50 = self.metrics.box.map50\n",
        "        precision = np.mean(self.metrics.box.p)\n",
        "        recall = np.mean(self.metrics.box.r)\n",
        "\n",
        "        print(\"\\n## Model Evaluation Metrics\\n\")\n",
        "        print(\"| Metric | Value | Description |\")\n",
        "        print(\"|--------|-------|-------------|\")\n",
        "        print(f\"| mAP50-95 | {mAP50_95:.3f} | Mean Average Precision across IoU thresholds 0.50-0.95 |\")\n",
        "        print(f\"| mAP50 | {mAP50:.3f} | Mean Average Precision at IoU threshold 0.50 |\")\n",
        "        print(f\"| Precision | {precision:.3f} | Overall precision across all classes |\")\n",
        "        print(f\"| Recall | {recall:.3f} | Overall recall across all classes |\")\n",
        "\n",
        "    def display_class_specific_metrics(self):\n",
        "        print(\"\\n## Class-specific Performance\\n\")\n",
        "        print(\"| Class | Precision | Recall | mAP50 | mAP50-95 |\")\n",
        "        print(\"|-------|-----------|--------|-------|----------|\")\n",
        "        for i, (p, r, map50, map) in enumerate(zip(self.metrics.box.p, self.metrics.box.r,\n",
        "                                                   self.metrics.box.ap50, self.metrics.box.ap)):\n",
        "            print(f\"| class_{i} | {p:.3f} | {r:.3f} | {map50:.3f} | {map:.3f} |\")\n",
        "\n",
        "    def display_speed_metrics(self):\n",
        "        print(\"\\n## Speed Metrics\\n\")\n",
        "        print(f\"- Preprocess time: {self.metrics.speed['preprocess']:.1f}ms per image\")\n",
        "        print(f\"- Inference time: {self.metrics.speed['inference']:.1f}ms per image\")\n",
        "        print(f\"- Postprocess time: {self.metrics.speed['postprocess']:.1f}ms per image\")\n",
        "\n",
        "    # def display_plot(self, plot_name, description):\n",
        "    #     img = cv2.imread(self.plots_path + plot_name)\n",
        "    #     resized_img = cv2.resize(img, (600, 400))\n",
        "    #     cv2_imshow(resized_img)\n",
        "    #     print(f\"\\n{description}\\n\")\n",
        "    def display_plot(self, plot_name, description):\n",
        "        img = cv2.imread(self.plots_path + plot_name)\n",
        "        resized_img = cv2.resize(img, (600, 400))\n",
        "        \n",
        "        # Convert from BGR to RGB color space\n",
        "        rgb_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Display using matplotlib\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(rgb_img)\n",
        "        plt.title(description)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        print(f\"\\n{description}\\n\")\n",
        "\n",
        "\n",
        "    def display_pr_curve(self):\n",
        "        self.display_plot(\"PR_curve.png\", \"The PR curve illustrates the trade-off between precision and recall at various confidence thresholds, indicating model performance across different operating points.\")\n",
        "\n",
        "    def display_precision_curve(self):\n",
        "        self.display_plot(\"P_curve.png\", \"The Precision curve shows how precision varies with confidence threshold for each class, helping identify optimal detection thresholds.\")\n",
        "\n",
        "    def display_recall_curve(self):\n",
        "        self.display_plot(\"R_curve.png\", \"The Recall curve demonstrates how recall changes with confidence threshold, showing the model's ability to find all relevant objects.\")\n",
        "\n",
        "    def display_confusion_matrix(self):\n",
        "        self.display_plot(\"confusion_matrix.png\", \"The confusion matrix displays the model's classification performance across all classes, showing true positives, false positives, and false negatives.\")\n",
        "\n",
        "    def display_validation_predictions(self):\n",
        "        self.display_plot(\"val_batch0_pred.jpg\", \"These images show the model's actual predictions on validation data, with bounding boxes and confidence scores.\")\n",
        "\n",
        "    def display_f1_curve(self):\n",
        "        self.display_plot(\"F1_curve.png\", \"The F1 curve shows the balance between precision and recall across different confidence thresholds for each class.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaSvXrvGiYhx"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfS6p0UzrgRR"
      },
      "source": [
        "We're implementing various datasets from roboflow and our own custom images dataset for maximum training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EU5ZM21jEYF"
      },
      "source": [
        "### Roboflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUZsO8gOk2BC",
        "outputId": "16e8f6f5-9294-4e84-f47b-112f13cba469"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Download both datasets\n",
        "# rf = Roboflow(api_key=api_key)\n",
        "\n",
        "# # Dataset 1\n",
        "\n",
        "# # project1 = rf.workspace(\"yassine-pzpt7\").project(\"sun-tracking-photovoltaic-panel\")\n",
        "# # version1 = project1.version(2)\n",
        "# # dataset1 = version1.download(\"yolov8\")\n",
        "\n",
        "# # # Dataset 2\n",
        "# # project2 = rf.workspace(\"1009727588-qq-com\").project(\"sun-nxvfz\")\n",
        "# # dataset2 = project2.version(2).download(\"yolov8\")\n",
        "\n",
        "# # # Dataset 3\n",
        "# # project3 = rf.workspace(\"rik-tjduw\").project(\"sun-tracking-555mn\")\n",
        "# # version3 = project3.version(4)\n",
        "# # dataset3 = version3.download(\"yolov8\")\n",
        "\n",
        "# # # Dataset 4\n",
        "# # project4 = rf.workspace(\"stardetect\").project(\"solar-re1fe\")\n",
        "# # version4 = project4.version(1)\n",
        "# # dataset4 = version4.download(\"yolov8\")\n",
        "\n",
        "# # # Dataset 5\n",
        "# # project5 = rf.workspace(\"fruitdetection-ulcz9\").project(\"sun_detection-hl04q\")\n",
        "# # version5 = project5.version(1)\n",
        "# # dataset5 = version5.download(\"yolov8\")\n",
        "\n",
        "# # Prepare dataset paths\n",
        "# dataset1_path = dataset1.location\n",
        "# dataset2_path = dataset2.location\n",
        "# dataset3_path = dataset3.location\n",
        "# dataset4_path = dataset4.location\n",
        "# # dataset5_path = dataset5.location\n",
        "\n",
        "# def resize_images(directory, target_resolution=(640, 640)):\n",
        "#     image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
        "#     for root, _, files in os.walk(directory):\n",
        "#         for file in files:\n",
        "#             if file.lower().endswith(image_extensions):\n",
        "#                 image_path = os.path.join(root, file)\n",
        "#                 try:\n",
        "#                     with Image.open(image_path) as img:\n",
        "#                         img_resized = img.resize(target_resolution, Image.LANCZOS)\n",
        "#                         img_resized.save(image_path)\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error resizing image {image_path}: {e}\")\n",
        "\n",
        "# # Function to count images in a directory\n",
        "# def count_images(directory):\n",
        "#     image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
        "#     return len([f for f in os.listdir(directory) if f.lower().endswith(image_extensions)])\n",
        "\n",
        "# # Function to combine datasets\n",
        "\n",
        "# def combine_datasets(*dataset_paths, combined_path):\n",
        "#     for split in ['train', 'valid', 'test']:\n",
        "#         os.makedirs(os.path.join(combined_path, split, 'images'), exist_ok=True)\n",
        "#         os.makedirs(os.path.join(combined_path, split, 'labels'), exist_ok=True)\n",
        "\n",
        "#         for dataset_path in dataset_paths:\n",
        "#             images_src = os.path.join(dataset_path, split, 'images')\n",
        "#             labels_src = os.path.join(dataset_path, split, 'labels')\n",
        "\n",
        "#             images_dst = os.path.join(combined_path, split, 'images')\n",
        "#             labels_dst = os.path.join(combined_path, split, 'labels')\n",
        "\n",
        "#             if os.path.exists(images_src):\n",
        "#                 for file in os.listdir(images_src):\n",
        "#                     shutil.copy2(os.path.join(images_src, file), images_dst)\n",
        "#             if os.path.exists(labels_src):\n",
        "#                 for file in os.listdir(labels_src):\n",
        "#                     shutil.copy2(os.path.join(labels_src, file), labels_dst)\n",
        "\n",
        "\n",
        "# # Combine datasets\n",
        "# combined_path = './roboflow_dataset_v3'\n",
        "# combine_datasets(dataset1_path, dataset2_path, dataset3_path,dataset4_path, combined_path=combined_path)\n",
        "\n",
        "# # Resize images in the combined dataset\n",
        "# for split in ['train', 'valid', 'test']:\n",
        "#     images_dir = os.path.join(combined_path, split, 'images')\n",
        "#     resize_images(images_dir)\n",
        "\n",
        "# print(\"Images resized to target resolution.\")\n",
        "\n",
        "\n",
        "# # Count images in each dataset\n",
        "# train_count = len(os.listdir(os.path.join(combined_path, 'train', 'images')))\n",
        "# valid_count = len(os.listdir(os.path.join(combined_path, 'valid', 'images')))\n",
        "# test_count = len(os.listdir(os.path.join(combined_path, 'test', 'images')))\n",
        "# num_classes = len(set([f.split('_')[0] for f in os.listdir(os.path.join(combined_path, 'train', 'labels'))]))\n",
        "\n",
        "# print(f\"Number of images in training set: {train_count}\")\n",
        "# print(f\"Number of images in validation set: {valid_count}\")\n",
        "# print(f\"Number of images in test set: {test_count}\")\n",
        "# print(f\"Total number of images: {train_count + valid_count + test_count}\")\n",
        "\n",
        "# # Create YAML file for YOLOv5\n",
        "# yaml_content = {\n",
        "#     'train': os.path.join(combined_path, 'train'),\n",
        "#     'val': os.path.join(combined_path, 'valid'),\n",
        "#     'test': os.path.join(combined_path, 'test'),\n",
        "#     'nc': len(os.listdir(os.path.join(combined_path, 'train', 'labels'))),\n",
        "#     'names': [f'class_{i}' for i in range(len(os.listdir(os.path.join(combined_path, 'train', 'labels'))))],\n",
        "#     'image_counts': {\n",
        "#         'train': train_count,\n",
        "#         'val': valid_count,\n",
        "#         'test': test_count\n",
        "#     }\n",
        "# }\n",
        "\n",
        "\n",
        "# yaml_content = {\n",
        "#     'train': os.path.join(combined_path, 'train'),\n",
        "#     'val': os.path.join(combined_path, 'valid'),\n",
        "#     'test': os.path.join(combined_path, 'test'),\n",
        "#     'nc': num_classes,\n",
        "#     'names': [f'class_{i}' for i in range(num_classes)],\n",
        "# }\n",
        "\n",
        "# yaml_path = os.path.join(combined_path, 'dataset.yaml')\n",
        "# with open(yaml_path, 'w') as yaml_file:\n",
        "#     yaml.dump(yaml_content, yaml_file, default_flow_style=False)\n",
        "\n",
        "# print(f\"Dataset YAML file created at: {yaml_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9gC344QjKpj"
      },
      "source": [
        "### Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Renaming Image Files Sequentially\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script renames all image files in a specified folder to sequential numbers (e.g., 1.jpg, 2.jpg, etc.). It uses a two-pass approach to avoid conflicts when renaming files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOa6B4brjI-I"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Specify the folder path\n",
        "# folder_path = r\"C:\\Users\\Som\\Desktop\\images\"\n",
        "\n",
        "# # List of common image file extensions\n",
        "# image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']\n",
        "\n",
        "# # Get all image files in the folder\n",
        "# image_files = [f for f in os.listdir(folder_path) if any(f.lower().endswith(ext) for ext in image_extensions)]\n",
        "\n",
        "# # Sort the image files to ensure consistent numbering\n",
        "# image_files.sort()\n",
        "\n",
        "# # First pass: Rename all files to temporary names\n",
        "# for index, filename in enumerate(image_files):\n",
        "#     temp_filename = f\"temp_{index}{os.path.splitext(filename)[1]}\"\n",
        "#     old_path = os.path.join(folder_path, filename)\n",
        "#     temp_path = os.path.join(folder_path, temp_filename)\n",
        "#     os.rename(old_path, temp_path)\n",
        "#     print(f\"Temporary rename: {filename} -> {temp_filename}\")\n",
        "\n",
        "# # Get the list of temporary files\n",
        "# temp_files = [f for f in os.listdir(folder_path) if f.startswith(\"temp_\")]\n",
        "# temp_files.sort()\n",
        "\n",
        "# # Second pass: Rename to final sequential numbers\n",
        "# for index, temp_filename in enumerate(temp_files, start=1):\n",
        "#     file_extension = os.path.splitext(temp_filename)[1]\n",
        "#     new_filename = f\"{index}{file_extension}\"\n",
        "#     temp_path = os.path.join(folder_path, temp_filename)\n",
        "#     new_path = os.path.join(folder_path, new_filename)\n",
        "#     os.rename(temp_path, new_path)\n",
        "#     print(f\"Final rename: {temp_filename} -> {new_filename}\")\n",
        "\n",
        "# print(f\"All {len(temp_files)} images have been renamed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Removing Images Without Corresponding Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script ensures that only images with corresponding label files (in the form of .txt files) are retained. Images without matching labels are removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # Specify the folder paths\n",
        "# labels_folder = r\"C:\\Users\\Som\\Desktop\\labels\"\n",
        "# images_folder = r\"C:\\Users\\Som\\Desktop\\images\"\n",
        "\n",
        "# # Get all txt files in the labels folder\n",
        "# txt_files = [os.path.splitext(f)[0] for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
        "\n",
        "# # Create a set for faster lookup\n",
        "# valid_names = set(txt_files)\n",
        "\n",
        "# # Go through the images folder\n",
        "# for filename in os.listdir(images_folder):\n",
        "#     name, ext = os.path.splitext(filename)\n",
        "#     if name not in valid_names:\n",
        "#         # Remove the image file\n",
        "#         file_path = os.path.join(images_folder, filename)\n",
        "#         os.remove(file_path)\n",
        "#         print(f\"Removed: {filename}\")\n",
        "\n",
        "# print(\"Finished removing images without corresponding labels.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Renaming Text Files Sequentially\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script renames all text files in a specified folder to sequential numbers (e.g., 1.txt, 2.txt, etc.). It uses a two-pass approach similar to the image renaming script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Specify the folder path for labels\n",
        "# folder_path = r\"C:\\Users\\Som\\Desktop\\labels\"\n",
        "\n",
        "# # Get all txt files in the folder\n",
        "# txt_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.txt')]\n",
        "\n",
        "# # Sort the txt files to ensure consistent numbering\n",
        "# txt_files.sort()\n",
        "\n",
        "# # First pass: Rename all files to temporary names\n",
        "# for index, filename in enumerate(txt_files):\n",
        "#     temp_filename = f\"temp_{index}.txt\"\n",
        "#     old_path = os.path.join(folder_path, filename)\n",
        "#     temp_path = os.path.join(folder_path, temp_filename)\n",
        "#     os.rename(old_path, temp_path)\n",
        "#     print(f\"Temporary rename: {filename} -> {temp_filename}\")\n",
        "\n",
        "# # Get the list of temporary files\n",
        "# temp_files = [f for f in os.listdir(folder_path) if f.startswith(\"temp_\")]\n",
        "# temp_files.sort()\n",
        "\n",
        "# # Second pass: Rename to final sequential numbers\n",
        "# for index, temp_filename in enumerate(temp_files, start=1):\n",
        "#     new_filename = f\"{index}.txt\"\n",
        "#     temp_path = os.path.join(folder_path, temp_filename)\n",
        "#     new_path = os.path.join(folder_path, new_filename)\n",
        "#     os.rename(temp_path, new_path)\n",
        "#     print(f\"Final rename: {temp_filename} -> {new_filename}\")\n",
        "\n",
        "# print(f\"All {len(temp_files)} txt files have been renamed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyPrpebpiqsh"
      },
      "source": [
        "## Setting Up Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31boAIzgs_CE"
      },
      "outputs": [],
      "source": [
        "yolo_model = YOLO('/mnt/c/Users/Som/Desktop/AI-Solar-Panel/runs/detect/v3_train/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_yaml_path = \"/mnt/c/Users/Som/Desktop/AI-Solar-Panel/datasets/roboflow_dataset_v3/dataset.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pBGiPvVpi3JA",
        "outputId": "09f42cb2-9629-431b-952f-0d6058860137"
      },
      "outputs": [],
      "source": [
        "# yolo_results = yolo_model.train(\n",
        "#     data=full_yaml_path,\n",
        "#     epochs=400,\n",
        "#     lr0=0.001,\n",
        "#     lrf=0.0001,\n",
        "#     batch=-1,\n",
        "#     device=[0],\n",
        "#     augment=True,\n",
        "#     cos_lr=True,\n",
        "#     amp=True,\n",
        "#     resume=True,\n",
        "#     patience=300,\n",
        "#     mixup=0.2,\n",
        "#     mosaic=1.0,\n",
        "#     close_mosaic=10\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNj_Fgq2lwVT"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaYezIxPlvmz"
      },
      "outputs": [],
      "source": [
        "# yolo_model.save('sun_tracker_v3.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yolo_model.export(\n",
        "    format='tflite',\n",
        "    device=\"cpu\"   \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yolo_model.save('sun_tracker_v3.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1dOULw1yXkK"
      },
      "source": [
        "## Evaluating Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_2bqOGxNi5In",
        "outputId": "57cbca7a-d0f0-4d8e-9817-a046309f1cdc"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model after training\n",
        "metrics = yolo_model.val(data=full_yaml_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Tz077XlYs4",
        "outputId": "f65813e9-cab0-4028-bc9c-8f6ad51bc04d"
      },
      "outputs": [],
      "source": [
        "# Assuming you have a metrics object and plots_path\n",
        "evaluator = ModelEvaluationDisplay(metrics, \"runs/detect/train/\")\n",
        "\n",
        "# Display various metrics and plots\n",
        "evaluator.display_overall_metrics()\n",
        "evaluator.display_class_specific_metrics()\n",
        "evaluator.display_speed_metrics()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "soPZae0v3602",
        "outputId": "beeba587-7740-4a55-ba9f-67eef923a13d"
      },
      "outputs": [],
      "source": [
        "evaluator.display_f1_curve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "V3SVHyE139Xy",
        "outputId": "83d4ef8e-afef-4580-aef4-0d888ef0c7aa"
      },
      "outputs": [],
      "source": [
        "evaluator.display_pr_curve()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "15_NUaCa4Otx",
        "outputId": "5e2563d8-ad20-45f8-dc6e-0587b253e11d"
      },
      "outputs": [],
      "source": [
        "evaluator.display_precision_curve()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "5djn7CSH4RIl",
        "outputId": "0fa5f779-49fa-42c1-af48-b278ed3726a4"
      },
      "outputs": [],
      "source": [
        "evaluator.display_recall_curve()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "-ML1ck3l4Snx",
        "outputId": "ed77e793-fdd8-4738-a28a-278fb10e794a"
      },
      "outputs": [],
      "source": [
        "evaluator.display_confusion_matrix()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "uD7ox68u4UVC",
        "outputId": "2127dbc2-c605-40c3-dba3-65002988fbd3"
      },
      "outputs": [],
      "source": [
        "evaluator.display_validation_predictions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Load model with GPU if available\n",
        "model = yolo_model\n",
        "\n",
        "def process_video(input_path, output_path, show_live=True):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "\n",
        "    # Get original video properties\n",
        "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    \n",
        "    # Target resolution for processing\n",
        "    target_size = 640\n",
        "    frame_skip = 2  # Process every other frame\n",
        "\n",
        "    # Set up video writer with TARGET resolution\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps//frame_skip, (target_size, target_size))\n",
        "\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        # Skip frames to reduce processing load\n",
        "        for _ in range(frame_skip - 1):\n",
        "            cap.grab()\n",
        "            \n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize once using optimized method\n",
        "        frame = cv2.resize(frame, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert to tensor with proper shape\n",
        "        if torch.cuda.is_available():\n",
        "            tensor_frame = torch.from_numpy(frame_rgb).permute(2, 0, 1).float().div(255).unsqueeze(0).to('cuda')\n",
        "        else:\n",
        "            tensor_frame = torch.from_numpy(frame_rgb).permute(2, 0, 1).float().div(255).unsqueeze(0)\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            results = model(tensor_frame, verbose=False)  # Pass tensor directly\n",
        "\n",
        "            \n",
        "        # Process results on CPU\n",
        "        annotated_frame = results[0].plot()\n",
        "        annotated_frame_bgr = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
        "        \n",
        "        # Write frame\n",
        "        out.write(annotated_frame_bgr)\n",
        "        \n",
        "        # Optional display (slows processing)\n",
        "        if show_live:\n",
        "            cv2.imshow('Solar Tracker', annotated_frame_bgr)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 10 == 0:\n",
        "            print(f\"Processed {frame_count} frames\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f\"Processing complete. Output saved to {output_path}\")\n",
        "\n",
        "# Usage\n",
        "process_video(\n",
        "    input_path='test/test.mp4',\n",
        "    output_path='output_video2.mp4',\n",
        "    show_live=False  # Disable for faster processing\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = yolo_model('test/1.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_yuSLTc14bn"
      },
      "outputs": [],
      "source": [
        "result = results[0]\n",
        "\n",
        "boxes = result.boxes  # Bounding boxes\n",
        "masks = result.masks  # Segmentation masks (if applicable)\n",
        "probs = result.probs  # Classification probabilities (if applicable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_vt8n2go2Avg",
        "outputId": "150041ff-bbc3-4508-ef6c-10b46df3e4ae"
      },
      "outputs": [],
      "source": [
        "plot = result.plot()\n",
        "\n",
        "\n",
        "\n",
        "# Convert BGR image to RGB for matplotlib\n",
        "\n",
        "plot_rgb = cv2.cvtColor(plot, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(plot_rgb)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Save the result\n",
        "cv2.imwrite(\"result_image_v3.jpg\", plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur2jM2ENqZPw"
      },
      "source": [
        "## Pretrained Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUAyyL24qe67"
      },
      "source": [
        "| Model | mAP50-95 | Precision | Recall | Images | Type | Suitability for Microcontroller |\n",
        "|-------|----------|-----------|--------|--------|------|-----------------------------------|\n",
        "| sun-tracking-555mn/4 | 97.7% | 93.0% | 89.2% | 923 | YOLOv8n | ✅ Excellent (Lightweight) |\n",
        "| solar-re1fe/1 | 96.8% | 95.2% | 94.3% | 2684 | Roboflow 3.0 | ✅ Good (Optimized for Edge) |\n",
        "| sun-tracking-photovoltaic-panel/1 | 98.2% | 93.7% | 93.7% | 196 | Roboflow 2.0 | ⚠️ Limited Dataset |\n",
        "| **Our YOLO model v3** | 73.8% | 91.7% | 92.4% | 633 | YOLOv8 | ✅ Good Moderate |\n",
        "| sun-tracking/3 | 92.5% | 94.7% | 91.8% | 1090 | YOLOv5 | ✅ Compatible (Proven on MCUs) |\n",
        "| Our YOLO model v2 | 42.6% | 78.7% | 79.3% | 274 | YOLOv8 | ❌ Too Heavy |\n",
        "| Our YOLO model v1 | 21.3% | 58.0% | 64.0% | 198 | YOLOv8 | ❌ Not Viable |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzzP7mnAqiCS"
      },
      "source": [
        "1. **sun-tracking-555mn/4**  \n",
        "   - **Best Choice for Deployment**  \n",
        "   - 97.7% mAP at 93.0ms inference  \n",
        "   - YOLOv8n architecture (4.9MB) fits Teensy 4.1's 1MB Flash  \n",
        "   - Confirmed working with LibRT for ARM Cortex-M7\n",
        "\n",
        "2. **solar-re1fe/1**  \n",
        "   - **Backup Option**  \n",
        "   - Roboflow's optimized TF-Lite model  \n",
        "   - Requires 64KB RAM (fits most MCUs)  \n",
        "   - Pre-trained weights reduce development time\n",
        "\n",
        "3. **Our YOLO model v3**  \n",
        "   - **Custom Solution Potential**  \n",
        "   - 73.8% mAP50-95 (Custom Trained)  \n",
        "   - 55.5ms inference on RTX 4050  \n",
        "   - Needs:  \n",
        "     ```bash\n",
        "     yolo export model=best.pt format=tflite int8 \n",
        "     ```\n",
        "   - Current size: 87MB → Target: <5MB after quantization\n",
        "\n",
        "4. **sun-tracking/3**  \n",
        "   - **Baseline Reference**  \n",
        "   - YOLOv5s (14.0MB) requires STM32H7 series  \n",
        "   - Proven C++ implementation exists\n",
        "\n",
        "5. **Legacy Models (v1/v2)**  \n",
        "   - **Development History**  \n",
        "   - Demonstrates iterative improvement  \n",
        "   - Not suitable for deployment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "PHGO27yizR1t",
        "outputId": "e86f93d9-fb2c-4a94-9d4d-32b0e76bfaf7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # define the image url to use for inference\n",
        "# image_file = \"test/1.jpg\"\n",
        "# image = cv2.imread(image_file)\n",
        "\n",
        "# # load a pre-trained yolov8n model\n",
        "# model = get_model(model_id=\"sun-tracking-555mn/4\", api_key=api_key)\n",
        "\n",
        "# # run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\n",
        "# results = model.infer(image)[0]\n",
        "\n",
        "# # load the results into the supervision Detections api\n",
        "# detections = sv.Detections.from_inference(results)\n",
        "\n",
        "# # create supervision annotators\n",
        "# bounding_box_annotator = sv.BoxAnnotator()\n",
        "# label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "# # annotate the image with our inference results\n",
        "# annotated_image = bounding_box_annotator.annotate(\n",
        "#     scene=image, detections=detections)\n",
        "# annotated_image = label_annotator.annotate(\n",
        "#     scene=annotated_image, detections=detections)\n",
        "\n",
        "# # display the image\n",
        "# sv.plot_image(annotated_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf8EK6eAqmaQ"
      },
      "source": [
        "### Implementing Live Web Cam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW3FAbhSqw60"
      },
      "source": [
        "#### Central Grid Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzPtyy0Kq2H6"
      },
      "source": [
        "A central gride box which would determine the strongest position for the solar panel to position for maximum sunlight. The solar panel will follow and turn according to the distance between the central grid and the sun detected in camera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LbIA9byw-3_"
      },
      "source": [
        "![image-1.png](https://github.com/somwrks/AI-Solar-Panel/blob/main/image-1.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRaoloktqmHn"
      },
      "outputs": [],
      "source": [
        "# def draw_central_box(frame, box_size=100):\n",
        "#     height, width = frame.shape[:2]\n",
        "#     center_x, center_y = width // 2, height // 2\n",
        "#     top_left = (center_x - box_size // 2, center_y - box_size // 2)\n",
        "#     bottom_right = (center_x + box_size // 2, center_y + box_size // 2)\n",
        "#     cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n",
        "#     return top_left, bottom_right\n",
        "\n",
        "# def calculate_distance_to_box(sun_center, top_left, bottom_right):\n",
        "#     sun_x, sun_y = sun_center\n",
        "#     box_x_min, box_y_min = top_left\n",
        "#     box_x_max, box_y_max = bottom_right\n",
        "\n",
        "#     closest_x = max(box_x_min, min(sun_x, box_x_max))\n",
        "#     closest_y = max(box_y_min, min(sun_y, box_y_max))\n",
        "\n",
        "#     return sun_x - closest_x, sun_y - closest_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygd-rJlyrIDE"
      },
      "source": [
        "Applying different image filters for detecting sun in various environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnhjkQwRrNJ6"
      },
      "outputs": [],
      "source": [
        "# def apply_sun_filter(frame):\n",
        "#     # Convert to float32 for better precision in calculations\n",
        "#     img = cv2.convertScaleAbs(frame, alpha=1.2)\n",
        "\n",
        "#     # Convert to HSV for better control\n",
        "#     hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "#     #  contrast enhancement\n",
        "#     hsv[:,:,2] = cv2.convertScaleAbs(hsv[:,:,2], alpha=-0.40)\n",
        "\n",
        "\n",
        "#     # # Convert back to BGR\n",
        "#     filtered = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "#     return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwGznkiyDUQJ"
      },
      "source": [
        "Adding Sun's position to adjust detection frequency based on sun movement to optimize power usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u310GuaYDhab"
      },
      "outputs": [],
      "source": [
        "# def calculate_sun_movement(lat, lon, time1, time2):\n",
        "#     pos1 = get_position(time1, lat, lon)\n",
        "#     pos2 = get_position(time2, lat, lon)\n",
        "#     movement = math.sqrt((pos2['azimuth'] - pos1['azimuth'])**2 + (pos2['altitude'] - pos1['altitude'])**2)\n",
        "#     return math.degrees(movement)\n",
        "\n",
        "# def calculate_detection_interval(lat, lon, current_time):\n",
        "#     future_time = current_time + timedelta(minutes=5)\n",
        "#     movement = calculate_sun_movement(lat, lon, current_time, future_time)  # Check movement over 5 minutes\n",
        "#     if movement > 1:  # If sun moves more than 1 degree in 5 minutes\n",
        "#         return 60  # Check every minute\n",
        "#     elif movement > 0.5:  # If sun moves more than 0.5 degrees in 5 minutes\n",
        "#         return 180  # Check every 3 minutes\n",
        "#     else:\n",
        "#         return 300  # Check every 5 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1gYGA3GDmbb"
      },
      "source": [
        "Activating the camera only when needed for detection to conserve energy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSHga_YiDrBc"
      },
      "outputs": [],
      "source": [
        "# def turn_off_camera(cap):\n",
        "#     cap.release()\n",
        "#     print(\"Camera turned off.\")\n",
        "\n",
        "# def turn_on_camera():\n",
        "#     cap = cv2.VideoCapture(0)\n",
        "#     if not cap.isOpened():\n",
        "#         print(\"Failed to turn on the camera.\")\n",
        "#     else:\n",
        "#         print(\"Camera turned on.\")\n",
        "#     return cap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewyeKcBOrRAZ"
      },
      "source": [
        "### Setting up pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvCwhOnRrRpB"
      },
      "outputs": [],
      "source": [
        "# # Load the pre-trained model\n",
        "# model = get_model(model_id=\"sun-tracking-555mn/4\", api_key=api_key)\n",
        "\n",
        "# # Create supervision annotators\n",
        "# bounding_box_annotator = sv.BoxAnnotator()\n",
        "# label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "# # Get current location\n",
        "# g = geocoder.ip('me')\n",
        "# latitude, longitude = g.latlng\n",
        "\n",
        "# print(f\"Current location: {g.city}, {g.state}, {g.country}\")\n",
        "# print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
        "\n",
        "# # Initial sun detection\n",
        "# sun_detected = False\n",
        "# cap = turn_on_camera()\n",
        "\n",
        "# while not sun_detected and cap.isOpened():\n",
        "#     ret, frame = cap.read()\n",
        "#     if not ret:\n",
        "#         break\n",
        "\n",
        "#     filtered_frame = apply_sun_filter(frame)\n",
        "#     top_left, bottom_right = draw_central_box(filtered_frame)\n",
        "#     results = model.infer(filtered_frame)[0]\n",
        "#     detections = sv.Detections.from_inference(results)\n",
        "\n",
        "#     if len(detections) > 0:\n",
        "#         sun_detected = True\n",
        "#         for bbox in detections.xyxy:\n",
        "#             sun_center = ((bbox[0] + bbox[2])//2, (bbox[1] + bbox[3])//2)\n",
        "#             dx, dy = calculate_distance_to_box(sun_center, top_left, bottom_right)\n",
        "#             print(f\"Initial Sun Detection - Time: {time.strftime('%H:%M:%S')}, dx: {dx}, dy: {dy}\")\n",
        "#     else:\n",
        "#         print(\"Searching for sun...\")\n",
        "\n",
        "#     annotated_frame = bounding_box_annotator.annotate(scene=filtered_frame, detections=detections)\n",
        "#     annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "#     cv2.imshow(\"Initial Sun Detection\", annotated_frame)\n",
        "\n",
        "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         break\n",
        "\n",
        "# turn_off_camera(cap)\n",
        "\n",
        "# if sun_detected:\n",
        "#     print(\"Sun detected. Starting interval-based tracking.\")\n",
        "#     detection_interval = calculate_detection_interval(latitude, longitude, datetime.now(timezone.utc))\n",
        "#     last_detection_time = time.time()\n",
        "\n",
        "#     while True:\n",
        "#         current_time = time.time()\n",
        "\n",
        "#         if current_time - last_detection_time >= detection_interval:\n",
        "#             cap = turn_on_camera()\n",
        "#             if cap.isOpened():\n",
        "#                 ret, frame = cap.read()\n",
        "#                 if ret:\n",
        "#                     filtered_frame = apply_sun_filter(frame)\n",
        "#                     top_left, bottom_right = draw_central_box(filtered_frame)\n",
        "#                     results = model.infer(filtered_frame)[0]\n",
        "#                     detections = sv.Detections.from_inference(results)\n",
        "\n",
        "#                     if len(detections) > 0:\n",
        "#                         for bbox in detections.xyxy:\n",
        "#                             sun_center = ((bbox[0] + bbox[2])//2, (bbox[1] + bbox[3])//2)\n",
        "#                             dx, dy = calculate_distance_to_box(sun_center, top_left, bottom_right)\n",
        "#                             print(f\"Time: {time.strftime('%H:%M:%S')}, dx: {dx}, dy: {dy}\")\n",
        "\n",
        "#                             detection_interval = calculate_detection_interval(latitude, longitude, datetime.now(timezone.utc))\n",
        "#                     else:\n",
        "#                         print(\"Sun not detected\")\n",
        "\n",
        "#                     annotated_frame = bounding_box_annotator.annotate(scene=filtered_frame, detections=detections)\n",
        "#                     annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "#                     cv2.imshow(\"Filtered Detection\", annotated_frame)\n",
        "\n",
        "#                 turn_off_camera(cap)\n",
        "\n",
        "#             last_detection_time = current_time\n",
        "\n",
        "#         time.sleep(1)  # Sleep for 1 second to reduce CPU usage\n",
        "\n",
        "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#             break\n",
        "\n",
        "# cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "J6Ap_wt6itSN",
        "dFWtiZSWi6e1"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
